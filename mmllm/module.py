import torch.nn as nn  
import torch 
import torch.nn.functional as F
from torch.distributions import Normal, Distribution
import numpy as np 
from typing import (
    Dict,   
    Optional, 
    Union, 
    Tuple, 
    Any, 
    Callable, 
    List, 
)
from scvi.module import CVAE
from scvi.base import LossOutput
from transformers import (
    PreTrainedModel, 
    PreTrainedTokenizer, 
    PreTrainedTokenizerFast, 
    Blip2QFormerConfig, 
    Blip2QFormerModel, 
)
from transformers.generation.streamers import BaseStreamer
from transformers.modeling_outputs import (
    Seq2SeqLMOutput,  
    CausalLMOutput, 
)

ACTIVATION_FAMILY = {
    "gelu": F.gelu, 
    "relu": F.relu, 
    "tanh": F.tanh, 
    "sigmoid": F.sigmoid, 
    "selu": F.selu,
    "silu": F.silu,
}

def get_torch_activation(name: str) -> Callable[[torch.Tensor], torch.Tensor]:
    """Get the torch activation function by name (e.g, ``get_torch_activation("gelu")``)."""
    func = ACTIVATION_FAMILY.get(name, None)
    if callable(func):
        return func

    raise ValueError(f"Unknown activation function: {name}.")

class ResidualBlock(nn.Module):
    """
    A PyTorch module implementing a residual block. It is composed of a linear layer, an activation
    function, and a dropout layer. 

    Parameters
    ----------
    n_inputs: int
        The number of input features (input dimension) for the linear layer.
    n_outputs: int
        The number of output features (output dimension) for the linear layer. If a residual 
        connection is enabled, ``n_inputs`` must be equal to ``n_outputs``.
    activation_func: str, default "gelu"
        The name of the activation function to apply after the linear transformation. It should 
        be one of "gelu", "relu", "tanh", "sigmoid", "selu", or "silu".
    dropout_rate: float, default 0.1
        The probability of an element to be zeroed in the dropout layer applied after the 
        activation function.
    residual: bool, default True
        If True, the input will be added to the output (residual connection), assuming the 
        input and output dimensions are the same. If False, no residual connection is used.
    """
    def __init__(
        self, 
        n_inputs: int, 
        n_outputs: int, 
        activation_func: str = "gelu", 
        dropout_rate: float = 0.1, 
        residual: bool = True,
    ) -> "ResidualBlock":
        super(ResidualBlock, self).__init__()
        if residual:
            assert n_inputs == n_outputs, "Residual connection requires the same input and output dimensions"
        
        self.residual = residual
        self.linear = nn.Linear(n_inputs, n_outputs)
        self.activation = get_torch_activation(activation_func)
        self.dropout = nn.Dropout(dropout_rate)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Given the input ``x``, get the corresponding output of the residual block."""
        if self.residual:
            return x + self.dropout(self.activation(self.linear(x)))
        else:
            return self.dropout(self.activation(self.linear(x)))

class SCQFormer(Blip2QFormerModel):
    """
    A variant of the BLIP2 Q-Former model designed for encoding cell counts. 

    The SCQFormer takes in a set of input cell counts, passes them through a series of residual blocks 
    (cell encoders) to generate key-value pairs. Q-Former is used to pool the key-value pairs via query 
    tokens to generate the final output.

    Parameters
    ----------
    count_dim: int
        The dimensionality of the input cell counts. This corresponds to the number of input features.
    num_query_tokens: int
        The number of query tokens generated by the model. These tokens interact with the hidden states 
        from residual blocks through cross-attention layers and interact with each other through 
        self-attention layers. 
    num_key_value_tokens: int
        The number of key-value tokens used in the cross-attention mechanism.
    config: transformers.Blip2QFormerConfig
        The configuration object that contains hyperparameters for the ``transformers.Blip2QFormer`` model.
    num_hidden_layers: int, default 2
        The number of ``ResidualBlock`` in the cell encoder.
    """
    def __init__(
        self, 
        count_dim: int, 
        num_query_tokens: int, 
        num_key_value_tokens: int, 
        config: Blip2QFormerConfig, 
        num_hidden_layers: int = 2, 
    ) -> "SCQFormer":
        # initialize the Blip2QFormerModel and model weights
        super().__init__(config)

        hidden_dropout_prob = config.hidden_dropout_prob
        hidden_act = config.hidden_act
        input_dim, output_dim = count_dim, num_key_value_tokens * config.hidden_size

        self.cell_encoder = []
        for i in range(num_hidden_layers):
            if i == 0:
                self.cell_encoder.append(
                    ResidualBlock(
                        input_dim,
                        output_dim, 
                        activation_func=hidden_act, 
                        dropout_rate=hidden_dropout_prob, 
                        residual=False,
                    )
                )
            elif i == num_hidden_layers - 1:
                self.cell_encoder.append(
                    ResidualBlock(
                        output_dim,
                        output_dim, 
                        activation_func=hidden_act, 
                        dropout_rate=0.0, 
                        residual=True,
                    )
                )
            else:
                self.cell_encoder.append(
                    ResidualBlock(
                        output_dim,
                        output_dim, 
                        activation_func=hidden_act, 
                        dropout_rate=hidden_dropout_prob, 
                        residual=True,
                    )
                )
        self.cell_encoder = nn.Sequential(*self.cell_encoder)
        self.query_tokens = nn.Parameter(torch.zeros(1, num_query_tokens, config.hidden_size))

        # initialize the weights of the model again 
        self.init_weights()
        nn.init.normal_(self.query_tokens, std=config.initializer_range)
    
    def forward(self, input_counts: torch.Tensor) -> torch.Tensor:
        """Encode the input cell counts and generate the final output."""
        encoder_hidden_states = self.cell_encoder(input_counts)
        batch_size = encoder_hidden_states.size(0)
        encoder_hidden_states = torch.reshape(encoder_hidden_states, (batch_size, -1, self.config.hidden_size))

        query_tokens = self.query_tokens.expand(batch_size, -1, -1)
        outputs = super().forward(query_tokens, encoder_hidden_states=encoder_hidden_states)[0]

        return outputs

class Generator(nn.Module):
    """
    A Generator model for conditional generation of gene expression profiles based on 
    conditional embeddings and latent variables. This model is built upon a Conditional 
    Variational Autoencoder (CVAE), which integrates continuous and categorical covariates 
    to generate synthetic data. 

    Parameters
    ----------
    n_input: int
        The dimensionality of the input data (i.e., the number of genes).
    condition_dim: int, default 4096
        The dimensionality of the condition embedding input, which represents the condition 
        that controls the generative process (e.g., cell type embeddings).
    condition_input_dim: int, default 128
        The dimensionality of the mapped condition after processing by the mapping layer.
    **kwargs: dict, optional
        Additional arguments passed to the base model, typically the ``scvi.module.CVAE`` model.
    """
    def __init__(
        self, 
        n_input: int, 
        condition_dim: int = 4096,
        condition_input_dim: int = 128, 
        **kwargs
    ) -> "Generator":

        super(Generator, self).__init__()
        self.mapping_layer = nn.Linear(condition_dim, condition_input_dim)
        kwargs["n_continuous_cov"] = condition_input_dim
        self.base_model = CVAE(n_input, **kwargs)
    
    def forward(
        self, 
        condition_embeds: torch.Tensor,
        gene_counts: torch.Tensor,  
        batch_ids: Optional[torch.Tensor] = None, 
        cat_covs: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        loss_kwargs: Optional[Dict[str, Any]] = None,
        compute_loss: bool = True, 
    ) -> Union[Tuple[Dict[str, torch.Tensor | Distribution | None], Dict[str, Distribution | None]], LossOutput]:
        """
        Perform a forward pass through the generator.

        Parameters
        ----------
        condition_embeds: torch.Tensor
            The condition embeddings control the generation process.
        gene_counts: torch.Tensor
            A tensor representing observed gene counts, which serves as input for training.
        batch_ids: torch.Tensor, optional, default None 
            The batch identifiers for each sample in the input data. If None, all samples are assigned 
            to batch 0. Another way to consider batch information is to use  ``condition_embeds`` to encode 
            batch information.
        cat_covs: torch.Tensor, optional, default None
            Categorical covariates related to the sample.
        labels: torch.Tensor, optional, default None 
            Class labels associated with the samples.
        loss_kwargs: dict, optional, default None 
            Additional keyword arguments passed to the loss function ``scvi.module.CVAE.loss`` (e.g., the weight of 
            the KL divergence term between latent variables and the prior distribution).
        compute_loss: bool, default True
            Whether to compute and return the loss during the forward pass. If False, the outputs of 
            inference process and generative process are returned. If True, the loss is returned.

        Returns
        -------
        outputs: Union[Tuple[Dict[str, torch.Tensor | Distribution | None], Dict[str, Distribution | None]], LossOutput]
            The outputs of the model. If ``compute_loss`` is True, the loss is returned. Otherwise, the outputs
            of the inference process and generative process are returned.
        """
        if condition_embeds.ndim > 2:
            condition_embeds = condition_embeds.squeeze(1)
        if batch_ids is None:
            batch_ids = torch.zeros((gene_counts.shape[0], 1)).to(
                dtype=torch.long, 
                device=gene_counts.device
            )
        inputs = {
            "gene_counts": gene_counts,
            "batch_ids": batch_ids,
            "cat_covs": cat_covs,
            "y": labels, 
        }
        cont_covs = self.mapping_layer(condition_embeds)
        inputs["cont_covs"] = cont_covs

        outputs = self.base_model(inputs, loss_kwargs=loss_kwargs, compute_loss=compute_loss) 
        if compute_loss:
            return outputs[-1]

        return outputs 

    @torch.inference_mode() 
    def generate(
        self, 
        condition_embeds: torch.Tensor, 
        batch_ids: Optional[torch.Tensor] = None,
        library_size: Optional[torch.Tensor] = None,
        batch_size: int = 128, 
        to_numpy: bool = True, 
    ) -> torch.Tensor | np.ndarray:
        """
        Generate synthetic gene expression profiles from the latent space.

        Parameters
        ----------
        condition_embeds: torch.Tensor
            The condition embeddings used to control the generation process.
        batch_ids: torch.Tensor, optional, default None 
            The batch identifiers for each sample in the input data. If None, all samples are assigned 
            to batch 0. Another way to consider batch information is to use  ``condition_embeds`` to encode 
            batch information.
        library_size: torch.Tensor, optional, default None 
            The library size vector predefined for each sample. It is used to control the library size of each 
            generated cell. If None, a prior distribution over library size will be used to sample the library
            size for each cell. Note that, in general, the prior distribution is set by 
            ``scvi.utils.init_library_size``.
        batch_size: int, default 128
            The batch size for generating data. Control the number of samples processed at once.
        to_numpy: bool, default True
            Whether to return the output as a NumPy array. If False, a PyTorch tensor is returned.

        Returns
        -------
        outputs: torch.Tensor | np.ndarray
            The generated gene expression data. The shape of the output depends on the input shape 
            and batch size. If ``to_numpy`` is True, the output is a NumPy array, otherwise it is a 
            PyTorch tensor.
        """
        if condition_embeds.ndim > 2:
            condition_embeds = condition_embeds.squeeze(1)
        cont_covs = self.mapping_layer(condition_embeds)
        n_samples = condition_embeds.shape[0] 
        if batch_ids is None:
            batch_ids = torch.zeros((n_samples, 1)).to(
                dtype=torch.long, 
                device=condition_embeds.device
            )
        device = next(self.base_model.parameters()).device

        outputs = [] 
        for i in range(0, n_samples, batch_size):
            inputs = {} 
            cont_covs_batch = cont_covs[i: i + batch_size] if cont_covs is not None else None 
            batch_ids_batch = batch_ids[i: i + batch_size]
            # the prior distribution of latent vectors 
            p_z = Normal(
                torch.zeros((batch_ids_batch.size(0), self.base_model.n_latent)), 
                torch.ones((batch_ids_batch.size(0), self.base_model.n_latent))
            )
            inputs = {
                "z": p_z.sample(),
                "batch_ids": batch_ids_batch,
                "cont_covs": cont_covs_batch,
            }
            if library_size is not None:
                inputs["library"] = library_size[i: i + batch_size]
            else:
                p_library = self.base_model.get_prior_library_distribution(batch_ids_batch)
                assert p_library is not None, "The model does not have a prior distribution for library size, please provide it"
                inputs["library"] = p_library.sample()
            inputs = {k: v.to(device) for k, v in inputs.items()}
            p_gene = self.base_model.generative(**inputs)["p_gene"]
            fake_samples = p_gene.sample() 
            if fake_samples.device.type == "cuda":
                fake_samples = fake_samples.cpu()
            outputs.append(fake_samples)
        outputs = torch.concat(outputs, dim=0) if len(outputs) > 1 else outputs[0]
        
        return outputs if not to_numpy else outputs.numpy()

class CellTextLLM(nn.Module):
    """
    A model class that integrates cell encoders, language models, and feature decoders.

    The model can merge encoded cell features with tokenized input text, generating both text and cell data.

    Parameters
    ----------
    base_model: transformers.PreTrainedModel or torch.nn.Module
        The underlying language model that will be used for text generation.
    tokenizer: transformers.PreTrainedTokenizer or transformers.PreTrainedTokenizerFast
        The tokenizer to tokenize the input text sequences.
    feature_encoder: transformers.PreTrainedModel or torch.nn.Module or None, optional, default None 
        A model used to encode input cell features, such as cell counts.
    feature_decoder: transformers.PreTrainedModel or torhc.nn.Module or None, optional, default None 
        A model used to decode cell features from the hidden states of the language model.
    normalize_total: bool, default True
        Whether to normalize the total input cell counts by dividing by the total sum of the counts.
    log_variational: bool, default True
        Whether to apply a log transformation (log1p) to the input cell counts before encoding. If 
        both normalization and log transformation are enabled, the log transformation is applied after
        normalization.
    """
    def __init__(
        self, 
        base_model: PreTrainedModel | nn.Module,
        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast, 
        feature_encoder: PreTrainedModel | nn.Module | None = None,
        feature_decoder: PreTrainedModel | nn.Module | None = None,
        normalize_total: bool = True, 
        log_variational: bool = True, 
    ) -> "CellTextLLM":
        super(CellTextLLM, self).__init__()
        self.base_model = base_model 
        self.tokenizer = tokenizer 
        if feature_encoder is not None:
            self.feature_encoder = feature_encoder
        else:
            self.register_buffer("feature_encoder", None)
        if feature_decoder is not None:
            self.feature_decoder = feature_decoder
        else:
            self.register_buffer("feature_decoder", None)
        
        assert hasattr(self.base_model, "config"),  \
            "Base model must have a config attribute. " + \
            "Please use prepare_cell_text_llm to prepare the model."
        assert hasattr(self.base_model.config, "eos_token_id"), \
            "Base model config must have eos_token_id attribute. " + \
            "Please use prepare_cell_text_llm to prepare the model."

        if self.feature_encoder is not None or self.feature_decoder is not None:
            assert hasattr(self.base_model.config, "special_tokens_index_dict"), \
                "Base model config must have special_tokens_index_dict attribute. " + \
                "Please use prepare_cell_text_llm to prepare the model."
            if self.feature_encoder is not None:
                assert hasattr(self.base_model.config, "pad_token_id"), \
                    "Base model config must have pad_token_id attribute. " + \
                    "Please use prepare_cell_text_llm to prepare the model."
                assert hasattr(self.base_model.config, "ignore_index"), \
                    "Base model config must have ignore_index attribute. " + \
                    "Please use prepare_cell_text_llm to prepare the model."
            if self.feature_decoder is not None:
                assert hasattr(self.base_model.config, "num_signal_tokens"), \
                    "Base model config must have num_signal_tokens attribute. " + \
                    "Please use prepare_cell_text_llm to prepare the model."
        
        self.normalize_total = normalize_total
        self.log_variational = log_variational

    def _merge_input_ids_with_features(
        self, 
        input_ids: torch.Tensor, 
        inputs_embeds: torch.Tensor, 
        features: torch.Tensor, 
        attention_mask: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
    ) -> Dict[str, torch.Tensor | Dict[str, torch.Tensor] | None]: 
        """
        Merge input token embeddings with encoded cell features and updates attention masks and labels accordingly.

        This method is designed to handle a mix of text and cell feature tokens. It identifies placeholder tokens in the 
        input text sequence (denoted by special tokens) and replaces them with corresponding cell features. It then constructs 
        the final sequence of embeddings that contains both text and feature embeddings, ensuring that the attention masks and 
        labels are updated accordingly for a consistent model input.

        Adapted from https://github.com/huggingface/transformers/blob/v4.40.1/src/transformers/models/llava/modeling_llava.py#L355. 

        Parameters
        ----------
        input_ids: torch.Tensor
            The input token IDs of shape (batch_size, sequence_length), where some tokens may be special placeholders 
            indicating the positions to insert cell features.
        inputs_embeds: torch.Tensor
            Precomputed embeddings corresponding to the ``input_ids`` of shape (batch_size, sequence_length, embed_dim).
        features: torch.Tensor
            Encoded cell features of shape (num_cells, num_features, embed_dim). These features will be inserted into 
            ``input_embeds`` based on the placeholders in ``input_ids``. If the features are 2D, they will be unsqueezed
            to 3D with an additional dimension.
        attention_mask: torch.Tensor
            Attention mask indicating which tokens should be attended to, of shape (batch_size, sequence_length).
        labels: torch.Tensor, optional, default None 
            Ground truth labels for token prediction tasks of shape (batch_size, sequence_length). These labels will also 
            be updated to match the new token positions after merging.

        Returns
        -------
        outputs: dict
            A dict containing:
            - "inputs_embeds": torch.Tensor
              The final merged embeddings with both text and cell feature embeddings of shape (batch_size, max_sequence_length, embed_dim).
            - "attention_mask": torch.Tensor
              The updated attention mask reflecting the new merged token sequence of shape (batch_size, max_sequence_length).
            - "labels": torch.Tensor, optional
              The updated labels of shape (batch_size, max_sequence_length), None if labels are not provided.

        Notes
        -----
        - This function assumes that the placeholders for cell features in the ``input_ids`` are identified by a special token. 
          The exact special token index is retrieved from the ``base_model.config.special_tokens_index_dict``. The final merged 
          sequence is padded to the maximum sequence length derived from the maximum number of input embeddings in the batch.
        - The number of placeholders in the input text sequence should match the number of cell features provided. 
        - For encoder-decoder models, the labels are not updated because the labels do not contain input tokens. 
        """
        # both MLP-based and transformer-based encoders are considered 
        if len(features.shape) == 2:
            features = features.unsqueeze(dim=1)

        num_features, num_feature_legnth, embed_dim = features.shape
        placeholder_index = self.base_model.config.special_tokens_index_dict["placeholder"]
        batch_size, sequence_length = input_ids.shape
        left_padding = not torch.sum(input_ids[:, -1] == torch.tensor(self.base_model.config.pad_token_id))

        # 1. create a mask to know where special placeholders are
        special_token_mask = input_ids == placeholder_index
        # for each sample, we calculate the number of placeholders for this modality 
        num_special_tokens = torch.sum(special_token_mask, dim=-1)
        # compute the maximum embed dimension
        # if the size of feature is equal to 1, the sequence length after merging is the same 
        # we just replace the placeholder with the corresponding features
        max_embed_dim = (num_special_tokens.max() * (num_feature_legnth - 1)) + sequence_length
        batch_indices, non_modality_indices = torch.where(input_ids != placeholder_index)

        # 2. compute the positions where text should be written
        # calculate new positions for text tokens in merged modality-text sequence
        # `special_token_mask` identifies placeholders
        # each placeholder will be replaced by `nb_text_tokens_per_feature` text tokens
        # `torch.cumsum` computes how each modality token shifts subsequent text token positions
        # - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one
        new_token_positions = torch.cumsum((special_token_mask * (num_feature_legnth - 1) + 1), -1) - 1
        nb_pad = max_embed_dim - 1 - new_token_positions[:, -1]
        if left_padding:
            new_token_positions += nb_pad[:, None]  # offset for left padding
        text_to_overwrite = new_token_positions[batch_indices, non_modality_indices]

        # 3. create the full embedding, already padded to the maximum position
        final_embedding = torch.zeros(
            batch_size, max_embed_dim, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device
        )
        final_attention_mask = torch.zeros(
            batch_size, max_embed_dim, dtype=attention_mask.dtype, device=inputs_embeds.device
        )
        if labels is not None and self.base_model.config.is_decoder:
            final_labels = torch.full(
                (batch_size, max_embed_dim), 
                self.base_model.config.ignore_index, 
                dtype=input_ids.dtype, 
                device=input_ids.device, 
            )
        # in case the modality encoder or the Language model has been offloaded to CPU, we need to manually
        # set the corresponding tensors into their correct target device
        target_device = inputs_embeds.device
        batch_indices, non_modality_indices, text_to_overwrite = (
            batch_indices.to(target_device),
            non_modality_indices.to(target_device),
            text_to_overwrite.to(target_device),
        )
        attention_mask = attention_mask.to(target_device)

        # 4. fill the embeddings based on the mask
        # if we have ["hey" "<CELL>", "how", "are"]
        # we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the cell features
        final_embedding[batch_indices, text_to_overwrite] = inputs_embeds[batch_indices, non_modality_indices]
        final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_modality_indices]
        if labels is not None and self.base_model.config.is_decoder:
            final_labels[batch_indices, text_to_overwrite] = labels[batch_indices, non_modality_indices] 
            
        # 5. fill the embeddings corresponding to the modality. Anything that is still zeros needs filling
        # B * L
        modality_to_overwrite = torch.all(final_embedding == 0, dim=-1)
        modality_to_overwrite &= modality_to_overwrite.cumsum(-1) - 1 >= nb_pad[:, None].to(target_device)
        if modality_to_overwrite.sum() != features.shape[:-1].numel():
            raise ValueError(
                f"The input provided to the model are wrong. The number of cell tokens is {torch.sum(special_token_mask)} while"
                f" the number of cells given to the model is {num_features}. This prevents correct indexing and breaks batch generation."
            )
        final_embedding[modality_to_overwrite] = features.contiguous().reshape(-1, embed_dim).to(target_device)
        final_attention_mask |= modality_to_overwrite
        
        # 6. mask out the embedding at padding positions, as we later use the past_key_value value to determine the non-attended tokens
        batch_indices, pad_indices = torch.where(input_ids == self.base_model.config.pad_token_id)
        indices_to_mask = new_token_positions[batch_indices, pad_indices]
        final_embedding[batch_indices, indices_to_mask] = 0
        if labels is None or self.base_model.config.is_encoder_decoder:
            # for encoder-decoder models, we don't need to update the labels
            # because the labels don't contain input tokens
            final_labels = None
        labels = final_labels
        attention_mask = final_attention_mask
        inputs_embeds = final_embedding
        
        return {
            "inputs_embeds": inputs_embeds,
            "attention_mask": attention_mask,
            "labels": labels,
        }
    
    def _get_condition_embeds(
        self, 
        last_hidden_states: torch.Tensor, 
        input_ids: torch.Tensor, 
        output_counts: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Extract conditional embeddings from the hidden states of the language model.

        This function is used to retrieve the embeddings corresponding to a set of signal tokens 
        in the input sequence. These signal tokens are used to indicate the presence of specific conditional information 
        (e.g., cell-related information). The embeddings of these tokens are extracted and used to generate gene profiles.

        Parameters
        ----------
        last_hidden_states: torch.Tensor
            The last hidden states of the language model's forward pass, of shape 
            (batch_size, sequence_length, hidden_size).
        input_ids: torch.Tensor
            The input token IDs of shape (batch_size, sequence_length), which include the special 
            tokens used to identify the locations of the conditional information.
        output_counts: torch.Tensor, optional, default None
            If provided, this tensor contains the expected output cell counts. It is used to verify
            that the number of samples containing signal tokens matches the number of expected outputs.

        Returns
        -------
        condition_embeds: torch.Tensor
            A tensor containing the extracted conditional embeddings from the hidden states, 
            of shape (num_cells, num_signal_tokens, hidden_size). These embeddings can be used 
            for generating gene profiles.

        Notes
        -----
        - This function assumes that the signal tokens in an input sequence are consecutive.
        - The number of samples containing signal tokens must match the number of expected conditional outputs 
        (e.g., gene counts) when provided.
        """
        # get conditional embeddings
        num_signal_tokens = self.base_model.config.num_signal_tokens
        first_signal_token_index = self.base_model.config.special_tokens_index_dict["first_signal_token"]
        output_ids = input_ids
        batch_indices, signal_token_indices = torch.where(output_ids == first_signal_token_index)
        if output_counts is not None and len(batch_indices) != len(output_counts):
            raise ValueError(
                f"The number of first signal tokens in inputs is {len(batch_indices)} "
                f"but the number of output counts you provide is {len(output_counts)}. "
                f"The row index of each signal token listed as follows:\n{batch_indices}"
            )
        indexer = signal_token_indices[:, None] + torch.arange(num_signal_tokens)[None].to(
            device=output_ids.device, 
            dtype=output_ids.dtype
        )
        target_ids = torch.arange(first_signal_token_index, first_signal_token_index + num_signal_tokens).to(
            device=output_ids.device, 
            dtype=output_ids.dtype
        )
        if not torch.all(torch.gather(output_ids[batch_indices], 1, indexer) == target_ids[None]):
            raise ValueError("The arrangement of signal tokens is not consecutive.")
        batch_indices = batch_indices[:, None].expand_as(indexer)
        condition_embeds = last_hidden_states[batch_indices, indexer]

        return condition_embeds

    @torch.inference_mode()
    def generate(
        self, 
        input_ids: torch.Tensor,
        input_counts: Optional[torch.Tensor] = None,
        return_conditions: bool = False, 
        attention_mask: Optional[torch.Tensor] = None,
        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,
        synced_gpus: Optional[bool] = None,
        assistant_model: Optional["PreTrainedModel"] = None,
        streamer: Optional["BaseStreamer"] = None,
        **kwargs,
    ) -> Dict[str, List[str | None | np.ndarray]]:
        """
        Generate texts or cells from text sequences ``input_ids`` and input counts ``input_counts`` (optionally). 

        This method combines tokenized text input with encoded cell features to generate both text 
        sequences and optionally cell data. It handles the integration of text and cell data using 
        a feature encoder and decoder if they are provided. 

        Parameters
        ----------
        input_ids: torch.Tensor
            Input token IDs of shape (batch_size, sequence_length) for the language model.
        input_counts: torch.Tensor, optional, default None
            Input counts of shape (num_cells, count_dim), which are passed through the feature encoder. 
            If not provided, text-only generation is performed.
        return_conditions: bool, optional, default False
            Whether to return the condition embeddings (i.e. the last hidden states of signal tokens).
        attention_mask: torch.Tensor, optional, default None
            A mask that specifies which tokens should be attended to. If not provided, it is computed from ``input_ids``.
        prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]], optional, default None 
            If provided, this function constraints the beam search to allowed tokens only at each step. If not provided, 
            no constraint is applied. Please see 
            https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate
            for more details. 
        synced_gpus: bool, optional, default None
            Whether to continue running the while loop until max_length. Please see 
            https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate
            for more details.
        assistant_model: PreTrainedModel, optional, default None 
            An assistant model that can be used to accelerate generation. The assistant model must have the exact same tokenizer. 
            The acceleration is achieved when forecasting candidate tokens with the assistent model is much faster than running 
            generation with the model you are calling generate from. As such, the assistant model should be much smaller.
            For more details, please see https://arxiv.org/abs/2302.01318. 
        streamer: BaseStreamer, optional, default None 
            Streamer object that will be used to stream the generated sequences. Generated tokens are passed through 
            ``streamer.put(token_ids)`` and the streamer is responsible for any further processing.
        **kwargs
            Additional keyword arguments passed to the ``generate`` method of the language model. In general, 
            the language model is based on the transformers library. Thus, please see 
            https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate
            for more details.

        Returns
        -------
        outputs: dict
            A dict containing the following keys:
            - "texts": list of str, the generated text sequences. Special tokens are removed.
            - "cells": list of cell data, where each entry is the decoded cell features. If no signal tokens are detected during 
            generation, each entry is None.
            - "conditions": list of numpy array, where each entry is the condition embeddings. If no signal tokens are detected during
            generation, each entry is None. This key is only present if ``return_conditions`` is True.
        """
        # we don't need custom stopping criteria because the built-in stopping criteria is good enough
        # we will use EosTokenCriteria by setting generation configuration's eos token id 
        # to the model's eos token id and the first signal token id 
        eos_token_id = [self.base_model.config.eos_token_id] 
        if self.feature_decoder is not None:
            eos_token_id.append(self.base_model.config.special_tokens_index_dict["first_signal_token"])

        if attention_mask is None:
            attention_mask =  (input_ids != self.base_model.config.pad_token_id).to(input_ids.device)
            attention_mask = attention_mask.long()
        inputs_embeds = self.base_model.get_input_embeddings()(input_ids)
        if input_counts is not None:
            if self.normalize_total:
                input_counts = input_counts / input_counts.sum(dim=-1, keepdim=True) * 1e4
            if self.log_variational:
                input_counts = torch.log1p(input_counts)
            assert self.feature_encoder is not None, "Feature encoder is not provided."
            features = self.feature_encoder(input_counts)
            merged_inputs = self._merge_input_ids_with_features(
                input_ids, 
                inputs_embeds, 
                features, 
                attention_mask, 
            )
            inputs_embeds = merged_inputs["inputs_embeds"]
            attention_mask = merged_inputs["attention_mask"]

        generated_ids = self.base_model.generate( 
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask, 
            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
            synced_gpus=synced_gpus,
            assistant_model=assistant_model,
            streamer=streamer,
            eos_token_id=eos_token_id,
            **kwargs, 
        ) 
        outputs = {"cells": [None] * generated_ids.size(0)}
        if return_conditions:
            outputs["conditions"] = [None] * generated_ids.size(0) 

        if self.feature_decoder is not None:
            # start to generate the cell 
            num_signal_tokens = self.base_model.config.num_signal_tokens
            first_signal_token_index = self.base_model.config.special_tokens_index_dict["first_signal_token"]
            batch_size, length = generated_ids.shape
            pad_token_id = self.base_model.config.pad_token_id
            cell_indices, signal_token_indices = torch.where(generated_ids == first_signal_token_index)
            if len(cell_indices) > 0:
                nb_pad = max(
                    (signal_token_indices + num_signal_tokens - 1).max().item() - length + 1, 0
                )
                if nb_pad > 0:
                    final_generated_ids = torch.concat(
                        [
                            generated_ids, 
                            torch.full((batch_size, nb_pad), pad_token_id).to(
                                device=generated_ids.device, 
                                dtype=generated_ids.dtype
                            )
                        ], 
                        dim=1
                    )
                else:
                    # deep copy because we will modify the previous generated tokens to remove the signal tokens 
                    final_generated_ids = generated_ids.clone()
                # remove the signal tokens 
                generated_ids[cell_indices, signal_token_indices] = pad_token_id
                shift = torch.arange(num_signal_tokens).to(
                    device=generated_ids.device,
                    dtype=generated_ids.dtype
                ) 
                indexer = signal_token_indices[:, None] + shift[None]
                final_generated_ids[cell_indices[:, None].expand_as(indexer), indexer] = shift[None] + first_signal_token_index

                # after the all signal tokens are added, we get the corresponding hidden states 
                if self.base_model.config.is_encoder_decoder:
                    decoder_input_ids = final_generated_ids
                    last_hidden_states = self.base_model(
                        inputs_embeds=inputs_embeds,
                        decoder_input_ids=decoder_input_ids, 
                        return_dict=True,
                        output_hidden_states=True, 
                    ).decoder_hidden_states[-1]
                    condition_embeds = self._get_condition_embeds(last_hidden_states, decoder_input_ids)
                elif self.base_model.config.is_decoder:
                    # unlike encoder-decoder architecture, the original input_ids are concatenated with the generated_ids
                    # note that we should consider padding tokens in the original input_ids
                    input_length = input_ids.size(-1)
                    new_input_ids = input_ids.clone()
                    num_normal_tokens = torch.sum(input_ids[cell_indices] != pad_token_id, dim=-1).to(dtype=torch.long)
                    max_num_normal_tokens = num_normal_tokens.max()
                    max_num_generated_tokens = (signal_token_indices.max() + num_signal_tokens)
                    # note that real nb_pad may be less than the calculated nb_pad
                    # because for an input with maximum length, its generated tokens may not be long enough to reach the maximum length 
                    # in this case, we do it for the convenience of parallel computing
                    # we will remove the redundant padding tokens later
                    nb_pad = max_num_normal_tokens.item() + max_num_generated_tokens.item() - input_length
                    if nb_pad > 0:
                        new_input_ids = torch.concat(
                            [
                                new_input_ids, 
                                torch.full((batch_size, nb_pad), pad_token_id).to(
                                    device=new_input_ids.device, 
                                    dtype=new_input_ids.dtype
                                )
                            ], 
                            dim=1
                        )
                    shift = torch.arange(0, max_num_generated_tokens.item()).to(
                        device=num_normal_tokens.device,
                        dtype=torch.long 
                    )
                    src_col_indexer = num_normal_tokens[:, None] + shift[None]
                    # concatenate the original input_ids with the generated_ids including the signal tokens
                    new_input_ids[cell_indices[:, None].expand_as(src_col_indexer), src_col_indexer] = final_generated_ids[cell_indices][0: max_num_generated_tokens.item()]
                    # remove the redundant padding tokens
                    new_input_ids = new_input_ids[:, torch.any(new_input_ids != pad_token_id, dim=0)]
                    new_inputs_embeds = self.base_model.get_input_embeddings()(new_input_ids)
                    last_hidden_states = self.base_model(
                        inputs_embeds=new_inputs_embeds,
                        return_dict=True,
                        output_hidden_states=True, 
                    ).hidden_states[-1]
                    condition_embeds = self._get_condition_embeds(last_hidden_states, new_input_ids)
                else:
                    raise ValueError("The model must be either encoder-decoder or decoder-only architecture.")
            
                # use downstream model to generate the cell
                cells = self.feature_decoder.generate(
                    condition_embeds, 
                    to_numpy=True
                )
                for cell_indice, cell in zip(cell_indices, cells):
                    outputs["cells"][cell_indice] = cell
                if return_conditions:
                    condition_embeds_ = condition_embeds
                    if condition_embeds_.device.type == "cuda":
                        condition_embeds_ = condition_embeds_.cpu() 
                    condition_embeds_ = condition_embeds_.numpy()
                    for cell_indice, condition in zip(cell_indices, condition_embeds_):
                        outputs["conditions"][cell_indice] = condition

        if generated_ids.device.type == "cuda":
            generated_ids = generated_ids.cpu() 
        outputs["texts"] = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

        return outputs 
    
    def forward(
        self, 
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        input_counts: Optional[torch.Tensor] = None,
        output_counts: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        generative_model_kwargs: Optional[Dict[str, Any]] = None, 
        **base_model_kwargs
    ) -> Seq2SeqLMOutput | CausalLMOutput:
        """
        Perform a forward pass for the CellTextLLM model, integrating cell feature generation 
        and text generation in a multimodal setup.

        Parameters
        ----------
        input_ids: torch.Tensor
            The token IDs of the input text sequence of shape (batch_size, sequence_length). 
        attention_mask: torch.Tensor, optional, default None
            A mask that indicates which tokens should be attended to, where 1 indicates a token should be attended 
            to and 0 indicates it should not be. If None, it is computed from ``input_ids``.
        input_counts: torch.Tensor, optional, default None
            A tensor of shape (num_cells, count_dim) representing the input cell counts to be processed by the feature encoder.
        output_counts: torch.Tensor, optional, default None
            A tensor representing expected output cell counts for supervision in the cell generation process.
        labels: torch.Tensor, optional, default None
            Ground truth labels for text generation. Used for supervised training when provided.
        output_attentions: bool, optional, default None
            If set to True, the model returns attention weights in addition to the output.
        output_hidden_states: bool, optional, default None
            If set to True, the model returns hidden states in addition to the output.
        generative_model_kwargs: dict, optional, default None
            Additional keyword arguments passed to the ``forward`` method of cell generative model if output cell counts 
            are provided.
        **base_model_kwargs: dict
            Additional keyword arguments passed to the ``foward`` method of base language model.

        Returns
        -------
        outputs: transformers.modeling_outputs.Seq2SeqLMOutput or transformers.modeling_outputs.CausalLMOutput
            The outputs of the model. 
        """
        # get text embeddings
        inputs_embeds = self.base_model.get_input_embeddings()(input_ids)
        if input_counts is not None:
            if self.normalize_total:
                input_counts = input_counts / input_counts.sum(dim=-1, keepdim=True) * 1e4
            if self.log_variational:
                input_counts = torch.log1p(input_counts)
        if attention_mask is None:
            attention_mask =  (input_ids != self.base_model.config.pad_token_id).to(input_ids.device)
            attention_mask = attention_mask.long()

        # get cell modality's features
        if input_counts is not None:
            assert self.feature_encoder is not None, "Feature encoder is not provided."
            features = self.feature_encoder(input_counts)
            merged_inputs = self._merge_input_ids_with_features(
                input_ids, 
                inputs_embeds, 
                features, 
                attention_mask, 
                labels=labels,
            )
            inputs_embeds = merged_inputs["inputs_embeds"]
            attention_mask = merged_inputs["attention_mask"]
            if merged_inputs["labels"] is not None:
                labels = merged_inputs["labels"]

        base_model_kwargs = {
            **base_model_kwargs, 
            "return_dict": True,
        }
        if output_counts is None:
            outputs = self.base_model(
                attention_mask=attention_mask, 
                inputs_embeds=inputs_embeds,  
                labels=labels, 
                output_attentions=output_attentions, 
                output_hidden_states=output_hidden_states, 
                **base_model_kwargs,
            )
            return outputs 
        base_model_kwargs = {
            **base_model_kwargs, 
            "output_hidden_states": True, 
        }
        outputs = self.base_model(
            attention_mask=attention_mask, 
            inputs_embeds=inputs_embeds,  
            labels=labels, 
            output_attentions=output_attentions, 
            **base_model_kwargs,
        ) 

        # get conditional embeddings
        if hasattr(outputs, "decoder_hidden_states"):
            if "decoder_input_ids" not in base_model_kwargs:
                raise ValueError(
                    "decoder_input_ids must be provided in base_model_kwargs when the encoder-decoder model "
                    " is used to generate cells."
                )
            last_hidden_states = outputs.decoder_hidden_states[-1]
            input_ids_ = base_model_kwargs["decoder_input_ids"]
        else:
            last_hidden_states = outputs.hidden_states[-1]
            input_ids_ = input_ids
        condition_embeds = self._get_condition_embeds(
            last_hidden_states, 
            input_ids_, 
            output_counts=output_counts,
        )
        if generative_model_kwargs is None:
            generative_model_kwargs = {}
        generative_outputs = self.feature_decoder(
            condition_embeds, 
            output_counts,  
            **generative_model_kwargs
        )
        outputs.loss = outputs.loss + generative_outputs.loss 
        
        return outputs
